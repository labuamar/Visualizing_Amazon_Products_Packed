{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary modules\n",
    "\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import time\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a session\n",
    "\n",
    "s=Service('D:/Downloads/chromedriver_win32/chromedriver.exe')\n",
    "driver = webdriver.Chrome(service=s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating amazon links for any search term\n",
    "\n",
    "def get_url(search_term):\n",
    "    template='https://www.amazon.com/s?k={}&ref=nb_sb_noss_1'\n",
    "    search_term= search_term.replace(' ','+')\n",
    "    return template.format(search_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.amazon.com/s?k=toys&ref=nb_sb_noss_1\n"
     ]
    }
   ],
   "source": [
    "#Testing get_url\n",
    "\n",
    "url = get_url('toys')\n",
    "print(url)\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grabbing Links for #of Amazon Listings using Search Term\n",
    "\n",
    "Links = []\n",
    "\n",
    "def get_links(search_term, list_name, max_links=30):\n",
    "    url=get_url(search_term)\n",
    "    driver.get(url)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    results = soup.find_all('a', {'class':'a-link-normal a-text-normal'})\n",
    "\n",
    "    for i in range (max_links):\n",
    "        item= results[i]\n",
    "        href1 = item.get('href')\n",
    "        if 'https' not in href1:\n",
    "            url1= 'https://www.amazon.com' + item.get('href')\n",
    "        else:\n",
    "            url1= href1\n",
    "        list_name.append(url1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_links(search_term='toys', list_name=Links)\n",
    "get_links(search_term='gifts', list_name=Links)\n",
    "get_links(search_term='electronics', list_name=Links)\n",
    "get_links(search_term='lego set', list_name=Links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results = []\n",
    "\n",
    "def get_dimensions(list_name, csv_name):\n",
    "    for item in list_name:\n",
    "        driver.get(item)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        try:\n",
    "            title = soup.find('span', {'class':'a-size-large product-title-word-break'}).text.strip()\n",
    "        except AttributeError:\n",
    "            continue\n",
    "        try:\n",
    "            dimensionsxpath= '//*[@id=\"productDetails_detailBullets_sections1\"]/tbody/tr[1]/td'\n",
    "            dimensions = driver.find_element(By.XPATH, dimensionsxpath).text.strip()\n",
    "        except NoSuchElementException or AttributeError:\n",
    "            dimensions = ''\n",
    "        try:\n",
    "            weightxpath= '//*[@id=\"productDetails_detailBullets_sections1\"]/tbody/tr[2]/td'\n",
    "            weight = driver.find_element(By.XPATH, weightxpath).text.strip()\n",
    "        except NoSuchElementException or AttributeError:\n",
    "            weight = ''\n",
    "        result = [title, dimensions, weight]\n",
    "        csv_name.append(result)\n",
    "        # time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dimensions(list_name=Links,csv_name=Results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('amazon_dimensions_weight_102621_2.csv','w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['Title','Dimensions','Weight'])\n",
    "    writer.writerows(Results)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "07f157cfe799f9992aa6eae7fa5b966a1bd7109392fbdb4fc77ca3f34952d570"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit (system)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
